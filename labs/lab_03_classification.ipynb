{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 ‒  Classification\n",
    "\n",
    "\n",
    "**Group ID:** 44\n",
    "\n",
    "**Author 1 (sciper):** Paul Véronique Guillaume DEVIANNE (311030)\n",
    "**Author 2 (sciper):** Rizhong LIN (366842)\n",
    "\n",
    "**Release date:** 19.04.2023  \n",
    "**Due date:** 05.05.2023 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use PyTorch. If you are not familiar with this library, [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) is a quick tutorial of the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "Requirement already satisfied: torch==1.8.1 in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: torchvision==0.9.1 in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (0.9.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (from torch==1.8.1) (4.1.1)\r\n",
      "Requirement already satisfied: numpy in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (from torch==1.8.1) (1.19.5)\r\n",
      "Requirement already satisfied: dataclasses in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (from torch==1.8.1) (0.8)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/rizhong/opt/anaconda3/envs/iapr/lib/python3.6/site-packages (from torchvision==0.9.1) (8.4.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.system())\n",
    "if platform.system() == \"Darwin\":\n",
    "    %pip install torch==1.8.1 torchvision==0.9.1\n",
    "else:\n",
    "    %pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Out-of-Distribution detection in colorectal cancer histology (12 points)\n",
    "\n",
    "Colorectal cancer is one of the most widespread cancers for men and women. Diagnosis complemented with prognostic and predictive biomarker information is essential for patient monitoring and applying personalized treatments. A critical marker is the tumor/stroma ratio in unhealthy tissues sampled from the colon. The higher the ratio, the more invasive the cancer is. The degree of invasion is tightly linked to patient survial probability.\n",
    "\n",
    "To measure the ratio, a pathologist needs to analyze the unhealthy tissue under a microscope and estimate it from a look. As the number of samples to analyze is huge and estimations are only sometimes precise, automatic recognition of the different tissue types in histological images has become essential. Such an automatic process requires the development of a multi-class classifier to identify the numerous tissues. As shown below, they are usually 8 tissue types to categorize: TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE and EMPTY (background).\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/lab-03-data/part1/kather16.svg\" width=\"1100\">\n",
    "    <center>\n",
    "    <figcaption>Fig1: Collection of tissue types in colorectal cancer histology (Kather-16)</figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "Up to this day, state-of-the-art methods use deep-learning-based supervised learning methods. A downfall of such an approach is the necessity to access a well-annotated training dataset. In histology, annotating data is difficult. It is time-consuming and requires the expertise of pathologists. Moreover, the annotator must label every tissue type while only two (TUMOR and STROMA) are interesting. \n",
    "\n",
    "\n",
    "Consequently, we propose another approach. In order to make the annotation task less tedious, we ask the annotator to label only the tissues of interest and dump the others. Then, we must train a binary classifier to automatically recognize these tissues at test time. In this part, you will implement the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance (3 points)\n",
    "\n",
    "Based on the abovementioned process, your task is to build a model that recognizes TUMOR (Label 0) and STROMA (Label 1) tissue types. Your model will be supervised by a training dataset containing TUMOR and STROMA annotations; note that all other tissues have been dropped.\n",
    "We will not ask you to train a deep-learning-based binary classifier from scratch. Instead, we provide excellent features (descriptors) of the images we extracted from a visual foundation model. (Note: As the nature of the foundation model is not part of this lecture, feel free to ask TAs if you are curious).\n",
    "\n",
    "Run the cell below to extract the provided train and test dataset. Each image is represented by a 768-d feature vector extracted from a visual foundation model. The train and test datasets contain feature vectors of 878 and 186 images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([186, 768])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Label mapping\n",
    "label_to_classname = {0: \"TUMOR\", 1: \"STROMA\"}\n",
    "\n",
    "# Train features and labels\n",
    "train_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_features.pth\"))\n",
    "train_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_labels.pth\"))\n",
    "\n",
    "# Test features and labels\n",
    "test_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_features.pth\"))\n",
    "test_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_labels.pth\"))\n",
    "\n",
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (2.5 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using minimum Mahalanobis distance.\n",
    "\n",
    "*Note:* You are not allowed to use any prebuilt Mahalanobis distance function. Additionally, ```torch.cov``` is not defined to compute the covariance matrix. You can use ```sklearn.covariance.LedoitWolf``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.linalg import inv\n",
    "\n",
    "# Separate features based on class labels\n",
    "tumor_features = train_features[train_labels == 0]\n",
    "stroma_features = train_features[train_labels == 1]\n",
    "\n",
    "# Calculate the mean for each class\n",
    "tumor_mean = tumor_features.mean(axis=0)\n",
    "stroma_mean = stroma_features.mean(axis=0)\n",
    "\n",
    "# Compute the covariance matrices using LedoitWolf\n",
    "cov_estimator = LedoitWolf()\n",
    "tumor_cov = cov_estimator.fit(tumor_features).covariance_\n",
    "stroma_cov = cov_estimator.fit(stroma_features).covariance_\n",
    "\n",
    "\n",
    "# Function to compute Mahalanobis distance\n",
    "def mahalanobis_distance(x, mean, cov_inv):\n",
    "    diff = x - mean\n",
    "    return np.sqrt(np.dot(np.dot(diff, cov_inv), diff))\n",
    "\n",
    "\n",
    "# Precompute inverse of the covariance matrices\n",
    "tumor_cov_inv = inv(tumor_cov)\n",
    "stroma_cov_inv = inv(stroma_cov)\n",
    "\n",
    "# Classify test features\n",
    "predicted_labels = []\n",
    "for test_feature in test_features:\n",
    "    # Compute Mahalanobis distance for each class\n",
    "    tumor_distance = mahalanobis_distance(test_feature, tumor_mean, tumor_cov_inv)\n",
    "    stroma_distance = mahalanobis_distance(test_feature, stroma_mean, stroma_cov_inv)\n",
    "\n",
    "    # Classify based on the minimum distance\n",
    "    predicted_labels.append(0 if tumor_distance < stroma_distance else 1)\n",
    "\n",
    "# Convert list to numpy array\n",
    "predicted_labels = np.array(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 points)** Compute the accuracy of your predictions with the test labels (```test_labels```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9785\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracy\n",
    "accuracy = np.sum(predicted_labels == test_labels) / len(test_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance (3 points)\n",
    "\n",
    "You will note that the test you run above is not really realistic. Like the training set, it contains only the TUMOR and STROMA tissue types. Nevertheless, at test time, the other tissues (Label -1) are also present and cannot be filtered by hand. Moreover, they cannot be recognized by the model as they are out of the training distribution (It is the consequence of the laziness of the annotators ;)). For this reason, it is essential to filter them out. This task is called Out-of-Distribution (OoD) detection. \n",
    "\n",
    "A simple way to do OoD detection is to compute for every test example an OoD-ness score which should be low for In-Distribution (ID) examples and high for OoDs. Then we define a threshold from which every example with an OoD-ness lying above is discarded, and those lying below are forwarded to the model for prediction. An example of OoD-ness score is the minimum Mahalanobis distance.\n",
    "\n",
    "Run the cell below to load a new test set containing OoD examples. It has 186 ID and 558 OoD examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([744, 768])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_classname_w_ood = {0: \"TUMOR\", 1: \"STROMA\", -1: \"OoD\"}\n",
    "\n",
    "# Test features and labels with OoD tissues\n",
    "test_features_w_ood = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test2_features.pth\"))\n",
    "test_labels_w_ood = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test2_labels.pth\"))\n",
    "\n",
    "test_features_w_ood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (0.5 point)** Why do you think the minimum Mahalanobis distance is a good OoD-ness score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The minimum Mahalanobis distance is a good OoD-ness score, because it measures the similarity of a test sample to the training samples while accounting for the correlations among features. A higher minimum Mahalanobis distance indicates that the test example is far from both TUMOR and STROMA classes in the feature space, making it more likely to be an OoD example. This distance can be used as a metric to distinguish between in-distribution and out-of-distribution samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 point)** Compute the minimum Mahalanobis distance for every test examples in ```test_features_w_ood``` with respect to the training features (```train_features```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute minimum Mahalanobis distance for every test example\n",
    "min_mahalanobis_distances = []\n",
    "for test_feature in test_features_w_ood:\n",
    "    tumor_distance = mahalanobis_distance(test_feature, tumor_mean, tumor_cov_inv)\n",
    "    stroma_distance = mahalanobis_distance(test_feature, stroma_mean, stroma_cov_inv)\n",
    "    min_distance = min(tumor_distance, stroma_distance)\n",
    "    min_mahalanobis_distances.append(min_distance.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (0.5 point)** Plot a histogram to show the difference between the Mahalanobis distance of TUMOR, STROMA and OoD tissue types and comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate distances based on class labels\n",
    "tumor_distances = [min_mahalanobis_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == 0]\n",
    "stroma_distances = [min_mahalanobis_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == 1]\n",
    "ood_distances = [min_mahalanobis_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == -1]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(tumor_distances, bins=20, alpha=0.5, label='TUMOR')\n",
    "plt.hist(stroma_distances, bins=20, alpha=0.5, label='STROMA')\n",
    "plt.hist(ood_distances, bins=20, alpha=0.5, label='OoD')\n",
    "plt.xlabel('Minimum Mahalanobis Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "In the histogram, we can see that the Mahalanobis distances of the OoD tissue types are generally higher than those of the TUMOR and STROMA tissue types. This suggests that the minimum Mahalanobis distance can be used as an OoD-ness score to differentiate between in-distribution and out-of-distribution samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Find a threshold on the Mahalanobis distance such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 30.7695\n",
      "Filtered TUMOR samples: 57\n",
      "Filtered STROMA samples: 69\n"
     ]
    }
   ],
   "source": [
    "threshold = np.percentile(ood_distances, 5)\n",
    "filtered_tumor = sum(d > threshold for d in tumor_distances)\n",
    "filtered_stroma = sum(d > threshold for d in stroma_distances)\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Filtered TUMOR samples: {filtered_tumor}\")\n",
    "print(f\"Filtered STROMA samples: {filtered_stroma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (0.5 point)** Assign prediction -1 to filtered out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```). Is it satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-wise accuracy:\n",
      "TUMOR: 0.3871\n",
      "STROMA: 0.2581\n",
      "OoD: 0.9498\n",
      "\n",
      "Overall accuracy: 0.7930\n",
      "In-distribution accuracy: 0.3226\n"
     ]
    }
   ],
   "source": [
    "# Assign prediction -1 to filtered out examples\n",
    "predicted_labels_w_ood = []\n",
    "for min_distance, test_feature in zip(min_mahalanobis_distances, test_features_w_ood):\n",
    "    if min_distance > threshold:\n",
    "        predicted_labels_w_ood.append(-1)\n",
    "    else:\n",
    "        tumor_distance = mahalanobis_distance(test_feature, tumor_mean, tumor_cov_inv)\n",
    "        stroma_distance = mahalanobis_distance(test_feature, stroma_mean, stroma_cov_inv)\n",
    "        predicted_labels_w_ood.append(0 if tumor_distance < stroma_distance else 1)\n",
    "\n",
    "# Compute average class-wise accuracy\n",
    "correct_predictions = 0\n",
    "total_tumor = sum(test_labels_w_ood == 0)\n",
    "total_stroma = sum(test_labels_w_ood == 1)\n",
    "total_ood = sum(test_labels_w_ood == -1)\n",
    "\n",
    "for pred, true_label in zip(predicted_labels_w_ood, test_labels_w_ood):\n",
    "    if pred == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy_tumor = sum(np.array(predicted_labels_w_ood)[test_labels_w_ood == 0] == 0) / total_tumor\n",
    "accuracy_stroma = sum(np.array(predicted_labels_w_ood)[test_labels_w_ood == 1] == 1) / total_stroma\n",
    "accuracy_ood = sum(np.array(predicted_labels_w_ood)[test_labels_w_ood == -1] == -1) / total_ood\n",
    "\n",
    "print(f\"Class-wise accuracy:\")\n",
    "print(f\"TUMOR: {accuracy_tumor:.4f}\")\n",
    "print(f\"STROMA: {accuracy_stroma:.4f}\")\n",
    "print(f\"OoD: {accuracy_ood:.4f}\")\n",
    "\n",
    "overall_accuracy = correct_predictions / len(test_labels_w_ood)\n",
    "print(f\"\\nOverall accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "in_distribution_accuracy = (sum(np.array(predicted_labels_w_ood)[test_labels_w_ood == 0] == 0) + sum(np.array(predicted_labels_w_ood)[test_labels_w_ood == 1] == 1)) / (total_tumor + total_stroma)\n",
    "print(f\"In-distribution accuracy: {in_distribution_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We do not find the results to be satisfactory. The overall accuracy stands at 0.79, while the in-distribution accuracy is merely 0.32. This implies that the model is struggling to classify the in-distribution samples accurately, as the threshold is set too low, causing most of the in-distribution samples to be filtered out.\n",
    "\n",
    "The current approach may result in a high false negative rate, wherein many TUMOR and STROMA samples are discarded as OoD samples, potentially impacting the clinical decision-making process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-distribution detection with k-NN classifier (6 points)\n",
    "\n",
    "The visual foundation models are known to be very good k-NN classifiers. It motivates us to implement a k-NN classifier to recognize TUMOR and STROMA. Moreover, k-NN distance is a good OoD-ness score and suits our task.\n",
    "\n",
    "**Task 1 (2 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using a k-NN classifier. Then report the accuracy of your predictions with the test labels (```test_labels```).\n",
    "\n",
    "*Note:* The choice of `k` is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Choose k\n",
    "k = 5\n",
    "\n",
    "# Train the k-NN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_classifier.fit(train_features, train_labels)\n",
    "\n",
    "# Make predictions on the test set\n",
    "knn_predictions = knn_classifier.predict(test_features)\n",
    "\n",
    "# Calculate the accuracy\n",
    "knn_accuracy = np.mean(knn_predictions == test_labels)\n",
    "print(f\"Accuracy: {knn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 points)** Perform OoD detection on the test features (```test_features_w_ood```) using a k-NN distance based OoD-ness score. Find a threshold on your OoD-ness score such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out? Finally, assign prediction -1 to filter out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```).\n",
    "\n",
    "*Note:* The OoD-ness is based on the distance to the k-nearest neighbors. The formulation is up to you. You have to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 43.8354\n",
      "Filtered TUMOR samples: 60\n",
      "Filtered STROMA samples: 45\n",
      "\n",
      "Class-wise accuracy:\n",
      "TUMOR: 0.3548\n",
      "STROMA: 0.5161\n",
      "OoD: 0.9498\n",
      "\n",
      "Overall accuracy: 0.8212\n",
      "In-distribution accuracy: 0.4355\n"
     ]
    }
   ],
   "source": [
    "def compute_classwise_accuracy(predictions, true_labels, label_to_classname):\n",
    "    classwise_accuracy = {}\n",
    "    for label in label_to_classname.keys():\n",
    "        class_indices = np.where(true_labels == label)[0]\n",
    "        class_predictions = predictions[class_indices]\n",
    "        class_true_labels = true_labels[class_indices]\n",
    "        class_accuracy = np.mean(class_predictions == class_true_labels)\n",
    "        classwise_accuracy[label] = class_accuracy\n",
    "    return classwise_accuracy\n",
    "\n",
    "\n",
    "# Compute k-NN distances for every test example\n",
    "knn_distances = knn_classifier.kneighbors(test_features_w_ood, return_distance=True)[0]\n",
    "\n",
    "# Compute the average distance to the k-nearest neighbors\n",
    "avg_knn_distances = np.mean(knn_distances, axis=1)\n",
    "\n",
    "# Separate distances based on class labels\n",
    "tumor_distances = [avg_knn_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == 0]\n",
    "stroma_distances = [avg_knn_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == 1]\n",
    "ood_distances = [avg_knn_distances[i] for i in range(len(test_labels_w_ood)) if test_labels_w_ood[i] == -1]\n",
    "\n",
    "# Find a threshold to filter out 95% of the OoD examples\n",
    "threshold = np.percentile(ood_distances, 5)\n",
    "\n",
    "# Filter out examples and compute the average class-wise accuracy\n",
    "filtered_tumor = sum(d > threshold for d in tumor_distances)\n",
    "filtered_stroma = sum(d > threshold for d in stroma_distances)\n",
    "print(f\"Threshold: {threshold:.4f}\")\n",
    "print(f\"Filtered TUMOR samples: {filtered_tumor}\")\n",
    "print(f\"Filtered STROMA samples: {filtered_stroma}\")\n",
    "\n",
    "# Assign prediction -1 to filtered out examples\n",
    "knn_predictions_w_ood = knn_classifier.predict(test_features_w_ood)\n",
    "knn_predictions_w_ood[avg_knn_distances > threshold] = -1\n",
    "\n",
    "# Compute the average class-wise accuracy\n",
    "class_wise_accuracy = compute_classwise_accuracy(knn_predictions_w_ood, test_labels_w_ood, label_to_classname_w_ood)\n",
    "print(\"\\nClass-wise accuracy:\")\n",
    "for label, acc in class_wise_accuracy.items():\n",
    "    print(f\"{label_to_classname_w_ood[label]}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall accuracy: {np.mean(knn_predictions_w_ood == test_labels_w_ood):.4f}\")\n",
    "in_distribution_accuracy = (sum(np.array(knn_predictions_w_ood)[test_labels_w_ood == 0] == 0) + sum(np.array(knn_predictions_w_ood)[test_labels_w_ood == 1] == 1)) / (total_tumor + total_stroma)\n",
    "print(f\"In-distribution accuracy: {in_distribution_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (1 point)** Is k-NN better than Mahalanobis distance ? Make an hypothesis for the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "k-NN can be better than Mahalanobis distance for this problem because it considers local relationships among samples in the feature space, which may capture more complex structures within the data. The Mahalanobis distance, on the other hand, assumes that the data follows a Gaussian distribution, which may not always hold true. Moreover, k-NN can adapt its decision boundaries based on the local density of samples, making it more flexible and robust to noise.\n",
    "\n",
    "From the accuracy results, we can see that k-NN performs slightly better than Mahalanobis distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Do you think we can suggest the approach presented in this exercise to compute TUMOR/STROMA ratio automatically ? Justify your thoughs. If not, suggest at least two ideas to improve it.\n",
    "\n",
    "*Note:* Annotating all the training dataset is not an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "The approach presented in this exercise might not be optimal for computing the TUMOR/STROMA ratio automatically, mainly due to the following reasons:\n",
    "\n",
    "1. The accuracy of the classifiers (Mahalanobis distance and k-NN) might not be sufficiently high for clinical decision-making, especially considering the presence of OoD samples that could impact the overall performance of the classifiers.\n",
    "\n",
    "2. The classifiers may filter out a significant number of TUMOR and STROMA samples, leading to false negatives and inaccurate TUMOR/STROMA ratio estimations.\n",
    "\n",
    "To improve the presented approach, we can consider the following:\n",
    "\n",
    "1. **Feature Engineering**: We can try to extract more informative features from the images, which could better represent TUMOR and STROMA tissues and potentially improve the performance of the classifiers. Additional features could also be derived from the existing feature set using dimensionality reduction techniques such as PCA or t-SNE.\n",
    "\n",
    "2. **Ensemble Methods**: Combining the predictions of multiple classifiers using techniques like bagging or boosting could potentially improve the overall classification performance. This approach can leverage the strengths of different classifiers, thereby reducing the impact of individual classifier weaknesses.\n",
    "\n",
    "3. **Semi-Supervised Learning**: Since annotating all the training dataset is not an option, we can use semi-supervised learning techniques, such as self-training or co-training, to leverage the information from both labeled and unlabeled data. This may help to improve the classifier's performance, particularly in the presence of OoD samples.\n",
    "\n",
    "These improvements, if implemented effectively, can lead to a more accurate and reliable computation of the TUMOR/STROMA ratio automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 (12 points)\n",
    "In this part, we aim to classify cervical cells resulting from Pap smear tests. To that end we'll be using a publicly available cell dataset: Sipakmed (https://www.cs.uoi.gr/~marina/sipakmed.html). The dataset is composed of 4049 images of isolated cells cropped from 966 cluster cell images of Pap smear slides. Each cell in the dataset has been categorized in either of the following categories: \n",
    "\n",
    "    - Superficial-Intermediate.\n",
    "    - Parabasal.\n",
    "    - Koilocytotic.\n",
    "    - Dysketarotic.\n",
    "    - Metaplastic.\n",
    "Your objective is to implement a classifier to automate the cell classification process. To ease your work we provide you with pre-computed embeddings for each images (`lab-03-data/part2/sipakmed_clean_embeddings.pth`). The embeddings are obtained from a pre-trained ResNet-50 (https://arxiv.org/pdf/1512.03385.pdf) and the corresponding images are also provided (`lab-03-data/part2/sipakmed_clean`). Note that you are free to discard the provided embeddings and work directy with the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset (4 points)\n",
    "Your first task is prepare the dataset such that it can be used to train your model. For that purpose we prepared the skeleton of the class `Sipakmed` that inherits from the class `Dataset` of PyTorch. Read the documentation (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) and complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features\n",
    "features_path = '../data/lab-03-data2023/part2/sipakmed_clean_embeddings.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sipakmed(Dataset):\n",
    "    phase_dict = {\n",
    "        'train': {'start': 0.0, 'stop': 0.5},\n",
    "        'val': {'start': 0.5, 'stop': 0.75},\n",
    "        'test': {'start': 0.75, 'stop': 1.0}\n",
    "    }\n",
    "    label_dict = {\n",
    "        'im_Superficial-Intermediate': 0,\n",
    "        'im_Parabasal': 1,\n",
    "        'im_Metaplastic': 2,\n",
    "        'im_Koilocytotic': 3,\n",
    "        'im_Dyskeratotic': 4\n",
    "    }\n",
    "\n",
    "    def __init__(self, features_path, phase):\n",
    "        super(Sipakmed, self).__init__()\n",
    "        # Store class attributes\n",
    "        self.phase = phase\n",
    "\n",
    "        # Collect the dataimport torch\n",
    "        import torch.nn.functional as F\n",
    "        import numpy as np\n",
    "        self.raw_data = torch.load(features_path)\n",
    "        self.features, self.labels, self.paths = self.collect_data()\n",
    "\n",
    "    def collect_data(self):\n",
    "        # Iterate over the dirs/classes\n",
    "        features, labels, paths = [], [], []\n",
    "        for dir_name, dir_dict in self.raw_data.items():\n",
    "            # Get the paths and embeddings\n",
    "            dir_paths, dir_embeddings = list(zip(*[(k, v) for k, v in dir_dict.items()]))\n",
    "\n",
    "            # Split\n",
    "            n = len(dir_paths)\n",
    "            np.random.seed(42)\n",
    "            permutations = np.random.permutation(n)\n",
    "            dir_paths = np.array(dir_paths)[permutations]\n",
    "            dir_embeddings = torch.stack(dir_embeddings)[permutations]\n",
    "            n_start = int(n * self.phase_dict[self.phase]['start'])\n",
    "            n_stop = int(n * self.phase_dict[self.phase]['stop'])\n",
    "            dir_embeddings = dir_embeddings[n_start: n_stop]\n",
    "            dir_paths = dir_paths[n_start: n_stop]\n",
    "\n",
    "            # Store\n",
    "            features.append(dir_embeddings)\n",
    "            paths.append(dir_paths)\n",
    "            dir_labels = torch.tensor([self.label_dict[p.split('/')[-2]] for p in dir_paths])\n",
    "            labels.append(dir_labels)\n",
    "\n",
    "        # Merge\n",
    "        features = torch.cat(features)\n",
    "        labels = torch.cat(labels)\n",
    "        paths = np.concatenate(paths)\n",
    "        return features, labels, paths\n",
    "\n",
    "    def __len__(self, ):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the embedding, label, and image path of queried index.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "        return embedding, label, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the implementation of `Sipakmed` completed, create 3 instances of the class (train/val/test) with the corresponding `phase` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-96416342b081>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-17-96416342b081>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    train_dataset = ### YOUR CODE\u001B[0m\n\u001B[0m                                 ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the datasets\n",
    "train_dataset = ### YOUR CODE\n",
    "val_dataset = ### YOUR CODE\n",
    "test_dataset = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your datasets are ready, use the class `DataLoader` from PyTorch to let it handle efficiently the batching, shuffling, etc. of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loaders\n",
    "train_loader = ### YOUR CODE\n",
    "val_loader = ### YOUR CODE\n",
    "test_loader = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to know your data. Plot a few example images for each class of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training example\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training (4 points)\n",
    "In this part your objective is to implement the required tools to train your model. The first thing you'll need is a a model which takes as input the pre-computed features and returns the corresponding class probabilities/logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the model\n",
    "embedding_dim = train_dataset.features.shape[1]\n",
    "model = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will keep track of your model's parameters, gradients, etc (https://pytorch.org/docs/stable/optim.html). It is responsible to update your model's parameters after each forward pass using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "optimizer = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss\n",
    "criterion = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that takes as input the model's output and the corresponding labels and returns the perçentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of predictions based on the model outputs (NxK: N samples, K classes) \n",
    "    and the labels (N: N samples).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `train` that forwards the complete training set through your model (= 1 epoch) and updates its parameters after each forward pass. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Reset the gradients\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Backpropagate\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Update the parameters\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "\n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "\n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `validate` that forwards the complete validation or test set through your model and evaluates its predictions. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    full_paths = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        l### YOUR CODE\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    full_paths = np.concatenate(full_paths)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses, full_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to train you model. Alternate between training and validation steps to find and save the best model (best accuracy on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "epochs = ### YOUR CODE\n",
    "best_acc = ### YOUR CODE\n",
    "model_savepath = '../data'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # Evaluate\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Save the model\n",
    "    if val_acc > best_acc:\n",
    "        ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation (4 points)\n",
    "Re-load the best model and evaluate its predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the best model\n",
    "### YOUR CODE\n",
    "\n",
    "# Evaluate\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool to analyze your model's performance on the different classes is the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Computes its entries for your model and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it can be useful to plot the problematic samples as well as the predicted and ground truth classes. Can you do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified samples\n",
    "### YOUR CODE\n",
    "\n",
    "# Plot the misclassified samples\n",
    "### YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
